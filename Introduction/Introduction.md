# A (Somewhat More) Practical Guide to Apache Spark

As distributed computing becomes a more commonplace solution [some intro here].

In the following article, we'll cover key concepts and practical examples that will help any data scientist make the transition from the more traditional, localized data science tools (pandas, Jupyter notebooks, sci-kit learn, etc.) to a distributed computing framework in Apache Spark.

Topics covered will include:

- Spark Internals and Core Spark Concepts (Master-Executor Framework, Partitions, Stages, and Jobs)
- Spark UI (SQL Plans, Executor/Job Metrics) 
- Spark Memory Management Basics 
- Various Data Serialization Formats

Please note, these articles are NOT meant to be a Spark programming tutorial. Though we will use and explain various code examples throughout, there are plenty of sources that cover the "how-tos" of Spark programming. These articles assume that you have a basic knowledge of Spark programming.

## Housekeeping Details

## Spark and the Distributed Computing Framework
